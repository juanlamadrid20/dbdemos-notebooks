{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b728819-2fad-4bc4-a879-ade17cc7eb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Feature Engineering Demo (Travel Purchase)\n",
    "\n",
    "This tutorial walks through an end-to-end workflow using the **latest Feature Engineering Declarative API (BETA)** [AWS](https://docs.databricks.com/aws/en/machine-learning/feature-store/declarative-apis) on Databricks.\n",
    "\n",
    "Weâ€™ll revisit the same use case â€” a **Travel Agency Recommender Model** â€” designed to increase revenue by personalizing travel and hotel offers based on each userâ€™s likelihood to make a purchase.\n",
    "\n",
    "---\n",
    "\n",
    " \n",
    "\n",
    "By the end of this notebook, youâ€™ll have a complete, production-ready feature engineering workflow â€” from data exploration to model scoring â€” all defined **declaratively and reproducibly**.\n",
    "\n",
    "\n",
    "## End-to-End Workflow\n",
    "\n",
    "1. **Explore & preprocess** raw tables registered in Unity Catalog  \n",
    "2. **Define feature sources** using `DeltaTableSource`  \n",
    "3. **Declare time-window features** with `create_feature`  \n",
    "4. **Assemble a unified training set** using point-in-time features \n",
    "5. **Train an ML model** to predict the `purchased` label  \n",
    "6. **Reuse the same features** for batch inference  \n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **Databricks Runtime:** 17.0 LTS ML or above   \n",
    "- **Serverless:** Not yet supported (coming soon)  \n",
    "\n",
    "---\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- View registered **feature tables** and **feature functions** in Unity Catalog  \n",
    "- Generate a **training dataset** enriched time-window features  \n",
    "- Train and evaluate a simple model predicting travel purchase likelihood  \n",
    "- Perform **batch inference** using the same declarative features  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f5cf58-be7f-4162-a201-4d5de5d17df6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the latest feature engineering"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Need to install the feature engineering library\n",
    "%pip install databricks-feature-engineering>=0.14.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69394700-3d64-44bd-b30e-7761838cfa8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering.entities import (\n",
    "    DeltaTableSource, Sum, Avg, SlidingWindow, Count, ContinuousWindow, TumblingWindow, StddevSamp, Max, Min, Window,\n",
    ")\n",
    "from datetime import timedelta\n",
    "fe = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e458b082-cb9e-4a55-b23a-b9b654184bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Explore raw data\n",
    "We start from two tables in Unity Catalog:\n",
    "- `travel_purchase`: event-level user interactions and purchases\n",
    "- `user_demography`: static attributes keyed by `user_id`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3348c1-e6ab-471d-a572-a5b46bcd6ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM travel_purchase limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de65f5bf-062d-467d-be53-6e1ddfae5521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1 Preprocess Raw Source Table\n",
    "\n",
    "Before we define features, every ML workflow begins with **data preprocessing** â€” the foundation of a reliable model.  \n",
    "In the declarative feature engineering API, preprocessing ensures that the raw source data is clean, typed correctly, and ready for numerical aggregations.\n",
    "\n",
    "In a typical ML lifecycle, preprocessing includes:\n",
    "- **Data cleaning** â€” handling missing values, outliers, or inconsistent formats  \n",
    "- **Feature type casting** â€” converting categorical or boolean fields into numeric forms suitable for aggregations  \n",
    "- **Key and timestamp alignment** â€” ensuring each record can be uniquely identified and temporally ordered  \n",
    "\n",
    "For this travel purchase use case, our raw behavioral event table (e.g., `user_clicks`) will be lightly transformed to meet the declarative APIâ€™s current expectations for **simple, numeric inputs**:\n",
    "\n",
    "- Cast `clicked` (**BOOLEAN**) â†’ `clicked_int` (**INT**) so it can be used in numeric aggregations  \n",
    "- Add a new column `impression` (constant value = 1) to enable **impression counts** using `sum()` aggregations  \n",
    "- Keep the original **primary keys** (`user_id`, `session_id`) and **timestamp column** (`event_time`) unchanged  \n",
    "\n",
    "These preprocessing steps create a consistent, analytics-friendly schema â€” enabling the `DeltaTableSource` and `create_feature()` API to compute robust and interpretable time-window features downstream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcb2064-c873-4d14-9577-3cd7281d2755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "cleaned_df = (\n",
    "    spark.table(f\"{catalog}.{schema}.travel_purchase\")\n",
    "         .withColumn(\"clicked_int\", F.col(\"clicked\").cast(\"int\"))\n",
    "         .withColumn(\"impression\", F.lit(1))  # for Count()\n",
    ")\n",
    "\n",
    "cleaned_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{catalog}.{schema}.travel_purchase_clean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "cefbabd9-73bb-4003-8021-726c2317187b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Define Feature Sources with `DeltaTableSource`\n",
    "\n",
    "In the declarative workflow, a **feature source** defines *where* the data comes from and *how* it should be interpreted for feature computation.  \n",
    "Each `DeltaTableSource` acts as a declarative reference to a Unity Catalog table â€” specifying its catalog, schema, keys, and time column â€” so features can later be derived reproducibly and governed under Unity Catalog lineage.\n",
    "\n",
    "For this use case, we define **two feature sources**:\n",
    "\n",
    "1. **User behavior features** â€” keyed by `user_id`, representing how each user interacts with travel listings  \n",
    "2. **Destination features** â€” keyed by `destination_id`, capturing aggregated statistics about travel destinations  \n",
    "\n",
    "Both use `event_time` as the **time column** to support time-windowed feature computations such as rolling averages or click counts.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_04_image7.png?raw=true\" style=\"float: right\" width=\"650px\" />\n",
    " \n",
    "### 2.1 Example\n",
    "\n",
    "The example below demonstrates how to define a feature source from a Unity Catalog table `main.analytics.user_events`:\n",
    "\n",
    "```python\n",
    "from databricks.feature_engineering.entities import DeltaTableSource\n",
    "\n",
    "user_behavior_source = DeltaTableSource(\n",
    "    catalog_name=\"main\",        # Catalog name\n",
    "    schema_name=\"analytics\",    # Schema name\n",
    "    table_name=\"user_events\",   # UC table name\n",
    "    entity_columns=[\"user_id\"], # Primary entity key(s)\n",
    "    timeseries_column=\"event_time\"  # Timestamp column for time-window features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e6eb2f-f2d0-4bd1-8efd-73e79dcf3216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source with entity keys at user_id level\n",
    "behavior_source = DeltaTableSource(\n",
    "    catalog_name=catalog,\n",
    "    schema_name=schema,\n",
    "    table_name=\"travel_purchase_clean\",\n",
    "    entity_columns=[\"user_id\"],\n",
    "    timeseries_column=\"ts\",\n",
    ")\n",
    "\n",
    "# Source with entity keys at destination_id level\n",
    "destination_source = DeltaTableSource(\n",
    "    catalog_name=catalog,\n",
    "    schema_name=schema,\n",
    "    table_name=\"travel_purchase_clean\",\n",
    "    entity_columns=[\"destination_id\"],\n",
    "    timeseries_column=\"ts\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431bc5f6-f653-460f-a355-8ed929a42b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Declare Time-Window Features with `create_feature`\n",
    "\n",
    "After defining feature sources, the next step is to **declare time-window features** â€” metrics that summarize user or system behavior over a defined period (e.g., last 7 days, last 30 days).  \n",
    "With the new declarative API, you can build these aggregations directly using `FeatureEngineeringClient.create_feature()`, which automatically handles validation, metadata registration, and lineage tracking in Unity Catalog.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4377ce-ac39-4d38-8660-f933fe43beed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 Concept\n",
    "\n",
    "A **time-window feature** aggregates a column of values across a rolling or continuous time window, producing features such as:\n",
    "- â€œTotal number of clicks in the past 7 daysâ€  \n",
    "- â€œAverage booking price over the last 30 daysâ€  \n",
    "- â€œMaximum spend in the last sessionâ€\n",
    "\n",
    "These temporal summaries capture recent trends in user behavior and are often the most predictive signals for recommender systems, fraud detection, or personalization models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af2b6268-fc14-435f-81a7-91bcfb01a83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  3.2 Supported Aggregation Functions\n",
    "\n",
    "> **Note:** All functions operate over the specified aggregation window.  \n",
    "> The window type (e.g., *sliding* or *continuous*) is defined in the feature pipeline configuration.\n",
    "\n",
    "| Function | Shorthand | Description | Example Use Case |\n",
    "|-----------|------------|-------------|------------------|\n",
    "| `Sum()` | `\"sum\"` | Total of all values within the window | Total minutes streamed per user |\n",
    "| `Avg()` / `Mean()` | `\"avg\"`, `\"mean\"` | Average value over the window | Mean transaction amount |\n",
    "| `Count()` | `\"count\"` | Number of records | Number of logins per user |\n",
    "| `Min()` | `\"min\"` | Minimum value observed | Lowest heart rate recorded |\n",
    "| `Max()` | `\"max\"` | Maximum value observed | Largest basket size per session |\n",
    "\n",
    "For additional supported functions (e.g., `stddev`, `count_distinct`, `approx_percentile`), see the  \n",
    "[Feature Engineering API documentation](https://docs.databricks.com/aws/en/machine-learning/feature-store/declarative-apis#supported-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bede1ec-8c4f-4c5c-a9af-b564ec40797b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Example\n",
    "\n",
    "The recommended and most robust approach is to use the  \n",
    "**`FeatureEngineeringClient.create_feature()`** method, which performs comprehensive validation and ensures correct feature registration.\n",
    "\n",
    "```python\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering.entities import DeltaTableSource, TimeWindow\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "feature = fe.create_feature(\n",
    "    source=DeltaTableSource(...),          # Required: DeltaTableSource or DataFrameSource\n",
    "    inputs=[\"clicked_int\"],                # Required: List of input columns\n",
    "    function=\"sum\",                        # Required: Aggregation function\n",
    "    time_window=TimeWindow(\"7d\"),          # Required: Aggregation window (e.g., 7 days)\n",
    "    catalog_name=\"main\",                   # Required: Target catalog\n",
    "    schema_name=\"features\",                # Required: Target schema\n",
    "    name=\"user_clicks_7d\",                 # Optional: Feature name (auto-generated if omitted)\n",
    "    description=\"Total user clicks in past 7 days\",  # Optional: Feature description\n",
    "    filter_condition=None                  # Optional: SQL filter for source data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5d4a2c43-689c-4141-9e90-642ffabfa659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4 Managing Feature Functions in Unity Catalog\n",
    "\n",
    "When you create a time-window feature using `create_feature()`, Databricks automatically registers a **feature function** in **Unity Catalog** under the specified schema.  \n",
    "This function encapsulates the featureâ€™s logic (e.g., input column, aggregation, and window definition) and allows consistent reuse across training and inference pipelines.\n",
    "\n",
    "If you **omit the `name` parameter** during creation, the system will automatically generate a descriptive name based on:\n",
    "- The **input column name**\n",
    "- The **aggregation function**\n",
    "- The **time-window specification**\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_04_image3.png?raw=true\" style=\"float: right\" width=\"350px\" />\n",
    "\n",
    "Over time, you may want to remove unused or experimental feature functions to keep your schema organized.  \n",
    "To delete a previously created feature function, use:\n",
    "\n",
    "```python\n",
    "fe.delete_feature_spec(\"<feature_function_name>\")\n",
    "```\n",
    "\n",
    "Pleae regularly clean up old or deprecated feature functions to keep your schema tidy, maintainable, and production-ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "9bce88ac-66dd-4e92-8801-358253486143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.5 Time Windows in Declarative Feature Engineering\n",
    "\n",
    "Time windows define **how far back in time** data is aggregated when computing a feature.  \n",
    "The Declarative Feature Engineering API supports multiple window types, each designed for different analytical or operational needs.\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_04_image5.png?raw=true\" style=\"float: right\" width=\"550px\" />\n",
    "\n",
    "\n",
    "- **Tumbling Window**: Tumbling windows divide time into **non-overlapping, consecutive intervals**, where each record belongs to exactly one window.  \n",
    "They are best suited for **periodic summaries** â€” for example, total bookings per week or average spend per month.\n",
    "\n",
    "- **Sliding Window**:Sliding windows define **overlapping intervals** that advance by a configurable step (the slide interval).  \n",
    "They are used for **rolling or moving aggregations**, such as a 7-day rolling average or a 30-day engagement rate recalculated daily.\n",
    "\n",
    "- **Continuous Window**: Continuous windows compute feature values **at a specific evaluation time**, aggregating data over a defined duration ending at that point.  \n",
    "They are ideal for **real-time** or **point-in-time** computations, such as streaming or online inference.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6abcd2dc-536e-407a-9ced-9ce65b9f456f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define User behavior features (per `user_id`)\n",
    "- `mean_price_7d`: 7-day rolling average of `price`\n",
    "- `mean_price_30d`: 30-day rolling average of `price`\n",
    "- `user_7d_clicks` : 7-day rolling sum of `clicked_int`\n",
    "- `user_7d_impression`: 7-day rolling sum of `impression`\n",
    "- `price_volatility_30d`: 30-day StddevSamp of `price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b122abd-f580-4212-9e47-d7e82545e2b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Declare behavior features"
    }
   },
   "outputs": [],
   "source": [
    "user_behavior_features = [\n",
    "\n",
    "    # 1) Short-horizon price intent (SLIDING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        #name=\"mean_price_7d\",\n",
    "        source=behavior_source,\n",
    "        inputs=[\"price\"],\n",
    "        function=Avg(),\n",
    "        time_window=SlidingWindow(\n",
    "            window_duration=timedelta(days=7),\n",
    "            slide_duration=timedelta(days=1),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # 2) Stable price intent (CONTINUOUS)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        #name=\"mean_price_30d_cont\",\n",
    "        source=behavior_source,\n",
    "        inputs=[\"price\"],\n",
    "        function=Avg(),\n",
    "        time_window=ContinuousWindow(\n",
    "            window_duration=timedelta(days=30),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # 3) User engagement via clicks (TUMBLING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        #name=\"user_clicks_7d_tumbling\",\n",
    "        source=behavior_source,\n",
    "        inputs=[\"clicked_int\"],           # 0/1\n",
    "        function=Sum(),\n",
    "        time_window=TumblingWindow(\n",
    "            window_duration=timedelta(days=7),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # 4) User activity via impressions (SLIDING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        #name=\"user_activity_7d\",\n",
    "        source=behavior_source,\n",
    "        inputs=[\"impression\"],            # always 1\n",
    "        function=Sum(),\n",
    "        time_window=SlidingWindow(\n",
    "            window_duration=timedelta(days=7),\n",
    "            slide_duration=timedelta(days=1),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "\n",
    "    # 5) User variability signal (SLIDING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        #name=\"price_volatility_30d\",\n",
    "        source=behavior_source,\n",
    "        inputs=[\"price\"],\n",
    "        function=StddevSamp(),\n",
    "        time_window=SlidingWindow(\n",
    "            window_duration=timedelta(days=30),\n",
    "            slide_duration=timedelta(days=1),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b50ffd66-c190-4e05-bd6e-1845426b0ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### For Debugging and Exploration - `compute_feature()` \n",
    "\n",
    "The `compute_feature()` method allows you to **preview feature values** by applying the declared feature logic directly to the source data.  \n",
    "Itâ€™s primarily designed for **debugging, validation, and exploratory analysis** â€” helping you verify that your feature definitions and aggregations behave as expected before materializing them with a pipeline.\n",
    "\n",
    "Typical uses include:\n",
    "- Inspecting computed feature values joined back to the raw source table  \n",
    "- Validating aggregation correctness or window logic  \n",
    "- Quickly testing transformations in development or notebook sessions  \n",
    "\n",
    "**Note**: `compute_feature()` is not intended for production use.\n",
    "It performs on-demand computation and does not persist or manage feature lineage in Unity Catalog.\n",
    "For production workflows, always use `create_pipeline()` to materialize features to Delta tables with governance, versioning, and scheduling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07d51c8-1227-4d65-a5c1-582e97740bd7",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765220125846}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Compute the features"
    }
   },
   "outputs": [],
   "source": [
    "feature_values = fe.compute_features(features=user_behavior_features)\n",
    "feature_values.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f731bd-1cb7-49ad-8eb8-96f890fb4a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Destination features (per `destination_id`)\n",
    "- `sum_clicks_7d`: 7-day rolling sum of `clicked_int` (number of clicks)\n",
    "- `sum_impressions_7d`: 7-day rolling sum of `impression` (number of impressions)\n",
    "- `dest_mean_price_7d`: 7-day rolling average of `price`\n",
    "- `dest_mean_price_30d_cont`: 30-day rolling average of `price`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1344928-f546-4b22-afec-e86d64590fd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Declare destination features"
    }
   },
   "outputs": [],
   "source": [
    "destination_features = [\n",
    "\n",
    "    # 6) Destination popularity via clicks (SLIDING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        name=\"dest_sum_clicks_7d\",\n",
    "        source=destination_source,\n",
    "        inputs=[\"clicked_int\"],            # 0/1\n",
    "        function=Sum(),\n",
    "        time_window=SlidingWindow(\n",
    "            window_duration=timedelta(days=7),\n",
    "            slide_duration=timedelta(days=1),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # 7) Destination exposure via impressions (TUMBLING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        name=\"dest_sum_impressions_7d_tumbling\",\n",
    "        source=destination_source,\n",
    "        inputs=[\"impression\"],             # 1\n",
    "        function=Sum(),\n",
    "        time_window=TumblingWindow(\n",
    "            window_duration=timedelta(days=7),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # 8) Recent destination pricing (SLIDING)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        name=\"dest_mean_price_7d\",\n",
    "        source=destination_source,\n",
    "        inputs=[\"price\"],\n",
    "        function=Avg(),\n",
    "        time_window=SlidingWindow(\n",
    "            window_duration=timedelta(days=7),\n",
    "            slide_duration=timedelta(days=1),\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # 9) Seasonal destination pricing (CONTINUOUS)\n",
    "    fe.create_feature(\n",
    "        catalog_name=catalog, schema_name=schema,\n",
    "        name=\"dest_mean_price_30d_cont\",\n",
    "        source=destination_source,\n",
    "        inputs=[\"price\"],\n",
    "        function=Avg(),\n",
    "        time_window=ContinuousWindow(\n",
    "            window_duration=timedelta(days=30),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713c0352-5422-43d5-9199-b56fd1994b91",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764722031481}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_values = fe.compute_features(features=destination_features)\n",
    "feature_values.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "3979ec8f-0b02-42cd-9524-287e6abe0827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Model Training with Declarative Feature Engineering (Point-in-Time Correct Features)\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_04_image10.png?raw=true\" style=\"float: right\" width=\"550px\" />\n",
    "In this section, we demonstrate how to build a machine learning model using the Declarative Feature Engineering API on Databricks.  \n",
    "\n",
    "### 4.1 `fe.create_training_set()`  \n",
    "This API automatically performs **point-in-time feature computation**, ensuring that every feature is joined using the correct values **as of the event timestamp** (for example, at purchase time or click time).  \n",
    "\n",
    "This eliminates common ML pitfalls such as data leakage, incorrect lookback windows, or manually managing temporal joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "357118aa-8403-487a-bc4f-bb7fba329e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2 About Non-Aggregation/Static Features (Current Limitation)\n",
    "\n",
    "The current release of the Declarative Feature Engineering API focuses on **time-window aggregation features** (e.g., sliding windows, tumbling windows, continuous windows).  \n",
    "At this time, the API **does not yet support transformation-only, non-aggregate features** such as:\n",
    "\n",
    "- gender \n",
    "- age\n",
    "- billing state, etc\n",
    "\n",
    "We intentionally keep the feature set clean and numeric, based purely on **rolling aggregations**, which are both powerful and easy to interpret. As Databricks adds support for **transformation features** in future releases,**we will seamlessly update this notebook** to include richer engineered features.\n",
    "\n",
    "In the cells that follow, we:\n",
    "\n",
    "1. Use `fe.create_training_set()` to assemble a training dataset with **point-in-time accurate** rolling features.\n",
    "2. Convert the training set into Pandas for classical ML workflows.\n",
    "3. Train a simple, interpretable ML model (RandomForest).\n",
    "4. Log the model and its feature lineage using `fe.log_model`, ensuring reproducibility and traceability.\n",
    "\n",
    "By the end, you will have a fully traceable ML model whose predictions can always be reproduced from the underlying **declarative feature definitions** â€” a cornerstone of production-grade ML systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6294f103-b5cf-4ee4-8101-13f4fe66cd47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the label dataset"
    }
   },
   "outputs": [],
   "source": [
    "id_and_label = spark.table('travel_purchase').select(\"id\", \"purchased\", \"destination_id\", \"user_id\", \"ts\").limit(1000)\n",
    "display(id_and_label.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afe0938-ee1b-4488-8c42-03cf471dd4b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Label Distribution"
    }
   },
   "outputs": [],
   "source": [
    "print(f\" Label distribution in training data:\")\n",
    "id_and_label.groupBy(\"purchased\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa566aa-583a-48e8-a5d8-765a1d546a68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the training set"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Create training set with point-in-time feature lookup\n",
    "training_set = fe.create_training_set(\n",
    "        name=\"travel_purchase_training_set_rolling_only\",\n",
    "        catalog_name=catalog,\n",
    "        schema_name=schema,\n",
    "        df=id_and_label,            \n",
    "        features=user_behavior_features + destination_features,    # rolling features (user + destination)\n",
    "        label=\"purchased\",\n",
    "        exclude_columns=[\"user_id\", \"destination_id\", \"ts\"],  \n",
    "    )\n",
    "\n",
    "print(\"Loading training DataFrame...\")\n",
    "training_df = training_set.load_df()\n",
    "print(f\"Spark DF rows: {training_df.count()}, cols: {len(training_df.columns)}\")\n",
    "display(training_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0878015e-6c8b-479f-8431-6e5270523d1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare features/label"
    }
   },
   "outputs": [],
   "source": [
    " # 2) Convert to Pandas and prepare features/label\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_pd = training_df.toPandas()\n",
    "\n",
    "# Drop label + keys/time if present\n",
    "drop_cols = [c for c in [\"purchased\", \"user_id\", \"destination_id\", \"ts\"] if c in training_pd.columns]\n",
    "X = training_pd.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "y = training_pd[\"purchased\"].astype(int).values.ravel()\n",
    "\n",
    "# Simple numeric fill for safety (rolling features are numeric)\n",
    "X = X.fillna(0)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_val:   {x_val.shape},   y_val:   {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac14257a-a916-4b12-b600-1c03ed8d2f98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Will include into init"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f4c1cf-0bfc-4a4a-b52a-497f823feb18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train & Log the model"
    }
   },
   "outputs": [],
   "source": [
    "# Define, train, evaule and log the model\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"/dbdemos_travel_rec\")\n",
    "\n",
    "model_name = \"dbdemos_fs_travel_dapi_rolling_only\"\n",
    "model_full_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"travel_rec_declarative_features\") as run:\n",
    "    \n",
    "    print(\"Training RandomForestClassifier inside MLflow run...\")\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(x_val)\n",
    "    if len(model.classes_) > 1:\n",
    "        y_prob = model.predict_proba(x_val)[:, 1]\n",
    "    else:\n",
    "        y_prob = np.zeros_like(y_pred, dtype=float)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_prob) if len(np.unique(y_val)) > 1 else 0.5\n",
    "\n",
    "    print(\" Validation Performance:\")\n",
    "    print(classification_report(y_val, y_pred, digits=3))\n",
    "    print(f\" AUC (val): {auc:.3f}\")\n",
    "\n",
    "    # Log metrics + artifacts\n",
    "    mlflow.log_metric(\"val_auc\", float(auc))\n",
    "    mlflow.log_metric(\"num_features\", int(X.shape[1]))\n",
    "    mlflow.log_metric(\"num_rows\", int(len(training_pd)))\n",
    "\n",
    "    # Feature importance\n",
    "    importances = model.feature_importances_\n",
    "    fi = dict(sorted(zip(list(X.columns), importances), key=lambda x: x[1], reverse=True))\n",
    "    mlflow.log_dict(fi, \"feature_importance.json\")\n",
    "\n",
    "    print(\"\\n Top Feature Importances:\")\n",
    "    for k, v in list(fi.items())[:15]:\n",
    "        print(f\" â€¢ {k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    # Log the model *with declarative feature lineage*\n",
    "    print(\"\\n Logging model to Unity Catalog with feature lineage...\")\n",
    "\n",
    "    fe.log_model(\n",
    "        model=model,\n",
    "        artifact_path=\"model\",\n",
    "        flavor=mlflow.sklearn,\n",
    "        training_set=training_set,        # links DFE features â†’ model lineage\n",
    "        registered_model_name=model_full_name\n",
    "    )\n",
    "\n",
    "print(f\"Registered model: {model_full_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5a8979df-31b6-4024-8ac3-34b709b073a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 4.3 Model lineage\n",
    "\n",
    "Lineage is automatically captured and visible within Unity Catalog. It tracks all tables up to the model created.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_04_image9.png?raw=true\" style=\"float: right\" width=\"650px\" />\n",
    "\n",
    "This makes it easy to track all your data usage, and downstream impact. If some PII information got leaked, or some incorrect data is loaded and detected by the Lakehouse Monitoring, it's then easy to track the potential impact.\n",
    "\n",
    "Note that this not only includes table and model, but also Notebooks, Dashboard, Jobs triggering the run etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6da7a75-2437-42b9-8f07-b1e14b9725f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the latest version of model"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "# Use the MlflowClient to get a list of all versions for the registered model in Unity Catalog\n",
    "all_versions = mlflow_client.search_model_versions(f\"name='{model_full_name}'\")\n",
    "# Sort the list of versions by version number and get the latest version\n",
    "latest_version = max([int(v.version) for v in all_versions])\n",
    "# Use the MlflowClient to get the latest version of the registered model in Unity Catalog\n",
    "latest_model = mlflow_client.get_model_version(model_full_name, str(latest_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dad7884-575e-4bde-96c1-485ddac26f7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Push the latest model into Prod"
    }
   },
   "outputs": [],
   "source": [
    "#Move it in Production\n",
    "production_alias = \"production\"\n",
    "if len(latest_model.aliases) == 0 or latest_model.aliases[0] != production_alias:\n",
    "  print(f\"updating model {latest_model.version} to Production\")\n",
    "  mlflow_client.set_registered_model_alias(model_full_name, production_alias, version=latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eeefffd-08a0-489b-8007-e72a92f6e781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.4 Running batch inferences\n",
    "\n",
    "We are now ready to run inferences.\n",
    "\n",
    "In a real world setup, we would receive new data from our customers and have our job incrementally refreshing our customer features running in parallel. \n",
    "\n",
    "To make the predictions, all we need to have is the primary keys for each feature loopup table. Feature Engineering in UC will automatically do the lookup for us as defined in the training steps.\n",
    "\n",
    "This is one of the great outcome using the Feature Engineering in UC: you know that your features will be used the same way for inference as training because it's being saved with your feature store metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb5393d-d923-4799-924e-ffb85302a200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the ids we want to forecast\n",
    "## For sake of simplicity, we will just predict using the same ids as during training, but this could be a different pipeline\n",
    "id_to_forecast = spark.table('travel_purchase').select(\"id\",\"user_id\",\"destination_id\", \"ts\")\n",
    "\n",
    "# Use result_type=\"double\" to get probability scores (values between 0 and 1)\n",
    "# Or use result_type=\"int\" to get class predictions (0 or 1)\n",
    "scored_df = fe.score_batch(model_uri=f\"models:/{model_full_name}@{production_alias}\", df=id_to_forecast, result_type=\"double\")\n",
    "display(scored_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "5862f179-bc19-4af4-b0e1-f30582a41855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5 Materialize Features with `create_pipeline()`\n",
    "\n",
    "After declaring individual features, use a **feature pipeline** to materialize them into an **offline feature table** for training and batch inference.  \n",
    "`FeatureEngineeringClient.create_pipeline()` builds a reproducible job that **backfills**, **schedules incremental runs**, and **writes managed Delta tables** in Unity Catalog.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_04_image2.png?raw=true\" style=\"float: right\" width=\"650px\" />\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 What it does\n",
    "- **Groups compatible features** into efficient execution plans\n",
    "- **Backfills** from a defined `start_time` (or auto-infers from source data)\n",
    "- **Schedules** recurring refreshes (e.g., hourly/daily) for rolling windows\n",
    "- **Materializes** results to a governed UC table (catalog/schema/prefix)\n",
    "\n",
    "### 5.2 BETA Limitation\n",
    "\n",
    "Today, the Feature Pipeline (in public preview) only supports **SlidingWindow** features for **incremental scheduling and refresh**.\n",
    "\n",
    "This means:\n",
    "\n",
    "- `ContinuousWindow` and `TumblingWindow` **will work in create_training_set()**,  \n",
    "  but **cannot yet be incrementally materialized** using a scheduled pipeline.\n",
    "- A Feature Pipeline will only accept features with **SlidingWindow** time windows.\n",
    "\n",
    "This limitation is temporary and will be expanded in future releases.\n",
    "\n",
    "\n",
    "### 5.2 Example\n",
    "\n",
    "```python\n",
    "FeatureEngineeringClient.create_pipeline(\n",
    "    features,                # List[Feature] â€“ features to materialize\n",
    "    offline_store_config,    # OfflineStoreConfig â€“ UC catalog/schema/table prefix\n",
    "    schedule=None,           # Optional[CronSchedule] â€“ refresh cadence\n",
    "    start_time=None          # Optional[datetime] â€“ earliest time to backfill from\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012f16a2-6960-483c-bfc1-90889ee01d5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter Sliding Features"
    }
   },
   "outputs": [],
   "source": [
    "# Filter only features with SlidingWindow (required for scheduled pipelines)\n",
    "Slidingfeatures = [\n",
    "    f for f in (user_behavior_features + destination_features)\n",
    "    if isinstance(f.time_window, SlidingWindow)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(Slidingfeatures)} features with SlidingWindow:\")\n",
    "for f in Slidingfeatures:\n",
    "    print(f\"  â€¢ {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fa7747-384b-45b4-bb2d-06a5c7eb8c68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create offline feature table for user behavior"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering.entities import OfflineStoreConfig\n",
    "from databricks.feature_engineering import CronSchedule\n",
    "offline_config = OfflineStoreConfig(\n",
    "    catalog_name=catalog,\n",
    "    schema_name=dbName,\n",
    "    table_name_prefix=\"travel_rec\"\n",
    ")\n",
    "fe.create_pipeline(\n",
    "    features=Slidingfeatures,\n",
    "    offline_store_config=offline_config,\n",
    "    schedule = None,\n",
    "    #schedule=CronSchedule(quartz_cron_expression=\"0 0 * * * ?\", timezone_id=\"UTC\"),   # run daily at 00:00,\n",
    "    start_time=None,\n",
    ")\n",
    "print (\"Pipeline created and executed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b64d18-a976-45fd-a14a-ebf92ca77b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸŽ‰ Wrapping Up: What's Next for Your Feature Engineering Workflow?\n",
    "\n",
    "Congratulations â€” youâ€™ve now completed an end-to-end walkthrough of Databricks **Declarative Feature Engineering**, including:\n",
    "\n",
    "- Defining time-aware rolling features using the new DFE API  \n",
    "- Generating **point-in-time correct** training sets with `fe.create_training_set()`  \n",
    "- Training an ML model with full **feature lineage**  \n",
    "- Materializing features using a **Feature Pipeline** for offline batch use cases\n",
    "\n",
    "This workflow provides a strong foundation for reproducible, governable, and production-ready feature engineering on the Lakehouse.\n",
    "\n",
    "## Next Steps: Online Feature Serving with LakeBase\n",
    "\n",
    "If your workspace have a **LakeBase instance** available, you can take your solution to the next level:\n",
    "\n",
    "### Publish features to an **Online Feature Store**  \n",
    "Using your existing feature definitions, you can push features to LakeBase for **low-latency, real-time inference** â€” exactly like we demonstrated in *Notebook 2*.\n",
    "\n",
    "This gives you:\n",
    "\n",
    "- Millisecond-level feature lookups  \n",
    "- Unified onlineâ€“offline consistency  \n",
    "- A clean path to powering real-time recommendation systems, fraud models, and more\n",
    "\n",
    "## Looking Ahead: Unified Offline + Online Publishing in Pipelines\n",
    "\n",
    "The current Feature Pipeline (in BETA) focuses on **offline feature materialization**. However, an upcoming release will add support for:\n",
    "\n",
    "### `offline_config` and `online_config`  \n",
    "within `create_pipeline()` â€” enabling:\n",
    "\n",
    "- Offline feature materialization (MV tables)\n",
    "- Online feature publishing (LakeBase)\n",
    "- Both **from the same declarative pipeline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0149ad7-fd1c-44dc-83a3-264273279bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 567797472004878,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04_declarative_feature_engineering(BETA)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
